{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage introduction\n",
    "> This notebook will introduce the steps to use the server api to decide if a new publication belongs to an author in academicworld database\n",
    "1. Connect to the UIUC VPN.\n",
    "2. Send a GET request to http://128.174.136.27:5000 with following parameters included:\n",
    "    + **target**: integer, the id of the author in academicworld database\n",
    "    + **title**: string, the title of the new publication\n",
    "    + **abstract**: string, the abstract for the new publication\n",
    "3. The feedback is a string in JSON format. The feedback contains the following fields:\n",
    "    + **co_author_score**: float, the score describe how likely the new publication is published by this author from the co-author perspective\n",
    "    + **co_author_conclusion**: boolean, according to the **co_author_score**, whether this new publication should belong to this author (emperical threshold: 0.65)\n",
    "    + **semantic_score**: float, the score describe how likely the new publication is published by this author from the semantic perspective\n",
    "    + **semantic_conclusion**: boolean, according to the **semantic_score**, whether this new publication should belong to this author (emperical threshold: 0.73)\n",
    "\n",
    "**Note**: \n",
    "+ Since querying the whole Microsoft Academic Graph for co-author measure is still too time consuming in real time, we use a smaller collection focused on publication in CS domain. Therefore, the **co_author_score** may be **not available** due to some \"author not found\" or \"publication not found\" cases. In those cases, the **co_author_score** will be *null* and **co_author_conclusion** will be *false*.\n",
    "+ On the other hand, **semantic_score** and **semantic_conclusion** will always be available if the inputs are all valid.\n",
    "\n",
    "Below is a piece of python code for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"co_author_conclusion\":true,\"co_author_score\":0.9317653973897299,\"semantic_conclusion\":true,\"semantic_score\":0.9998914003372192}\\n'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(url='http://128.174.136.27:5000', params={'target' : 3083, \n",
    "                                                           'title' : 'joint data purchasing and data placement in a geo distributed data market', \n",
    "                                                           'abstract' : 'This paper studies design challenges faced by a geo-distributed cloud data market: which data to purchase (data purchasing) and where to place/replicate the data (data placement). We show that the joint problem of data purchasing and data placement within a cloud data market is NP-hard in general. However, we give a provably optimal algorithm for the case of a data market made up of a single data center, and then generalize the structure from the single data center setting and propose Datum, a near-optimal, polynomial-time algorithm for a geo-distributed data market.',\n",
    "                                                           'coauthor' : str(['xiaoqi ren', 'juba ziani', 'adam wierman', 'palma london'])})\n",
    "print(r.content)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9c7e6a376f2381ce584cd33259e87a1795226bafbf9464c983b7a0ecc23664a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('test2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
